% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{rotating}
\usetikzlibrary{decorations.pathreplacing,shapes,arrows,positioning}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
\bibliographystyle{plainurl}

%
\title{Compiling Files in Parallel: A Study with GCC\thanks{Supported by CAPES and Google Summer of Code.}}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Giuliano Belinassi\inst{1} \and Jan Hubi\v cka \inst{2} \and Richard Biener\inst{2} \and Alfredo Goldman\inst{1}}
%
\authorrunning{G. Belinassi et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{Institute of Mathematics and Statistics, Rua do Matão 1010, São Paulo SP, BRA\\
\url{https://www.ime.usp.br} \and
SuSE Labs, Nürnberg 90409, GER\\
\url{https://www.suse.com/} \and
Charles University, Malostransk én ám. 25. 11800 Praha, CZE\\
\url{https://cuni.cz/uken-1.html}}

%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}

Processors are becoming increasingly parallel over time, but compiling
software has so far been a task parallelizable only by the number of
files in it. To improve compilation parallelism granularity, we propose
a method feasible to implement in commercial compilers for single file
parallel compilation, with simple modifications in the Link Time
Optimization (LTO) engine; which we show by implementing
it in GCC. This method resulted in a 35\% speedup when self-compiling
GCC in this mode, up to $3.5\times$ speedup when compiling individual files,
and we found no meaningful slowdown when compiling other applications.
We also briefly discuss how this impacts the Reproducible Builds project.

\keywords{Compilers \and Parallel Compilation \and Link Time Optimization \and LTO.}
\end{abstract}
%
%
%
\section{Introduction}

The recent advances in both technological and computational fields induced an
increasingly faster expansion of software ecosystems. Developers create new
programs to supply the needs of the most diverse domains, either through web
systems coded in script languages; or by components to an operating system
destined to control some hardware resources. Independently of the reason behind
the development of such systems, it is true that their code will be, in some
point, transformed into machine language by a compiler or assembler, even if it
is executed by an interpreter.

Compilers are enormous programs, largely adopted by industry and academy, where
a great effort was and still is employed so that they produce efficient code --
but without any sacrifice in correctness --. There are huge projects destined
to develop and improve them, such as the GNU Compiler Collections
(GCC\footnote{https://gcc.gnu.org/}) and LLVM\footnote{https://llvm.org/}, capable
of translate several languages such as C, C++, and Fortran, to machine language.

GCC was started by Richard Stallman, with the first public release in March of
1987. Back then, it
only supported C language, but already allowed code generation for several
architectures \cite{gcc-first-ver}. Initially designed to compile programs a
single file at time, it could not allow global cross-file optimizations because
the compiler never had the opportunity to analyze the program as a whole. This
changed when Link Time Optimization (LTO) was proposed \cite{whoprgoogle} and
implemented in GCC \cite{glek2010optimizing}. LTO can be enabled in GCC by
using \texttt{-flto}.

In this paper we will present in Section \ref{sec:related} the previous
efforts in compile a single file in parallel, as well as an introduction
to LTO. Then in Section \ref{sec:work} we present our methods for single
file compilation in details, as well as exposing some internal mechanisms
of GCC. Finally, we present our results in Section \ref{sec:results} and
how to improve from this paper in Section \ref{sec:future_works}.

\section{Related Works} \label{sec:related}

Parallel Compilation includes parsing (parallel or not),
how to perform analysis, optimization, and code translation in parallel.
Given an alphabet $\mathrm{\Sigma}$, parsing can be described as building a machine to decide if
an input string $w \in \mathrm{\Sigma}^*$ is a member of a certain language $L
\subseteq \mathrm{\Sigma}^*$ or not, creating the Abstract Syntax Tree in the
process by logging the used derivation rules.

Parallel Parsing dates back from the 1970. \cite{Lincoln:1970:PPT:987475.987478}
explored how to use the vectorial registers in the (so far)
STAR-100 supercomputer for lexical analysis.
\cite{fischer1975parsing} give a detailed theoretical study, proving
several concurrent parsing techniques for LR($k$), LALR($k$) and SLR($k$).
It proceeds by breaking the input in several arbitrary parts, and running a
serial parser on each of these parts. Then the algorithm tries to
recover the stack of each noninitial parser by constructing a set of
possible states, for which there are 5 possible cases. However, in case
of an error, the parser result should be discarded, and therefore a lot
of work will be done in vain when comparing with the sequential version.

But perhaps the most interesting work is by
\cite{Barenghi:2015:PPM:2839536.2840146}, where they explore properties of
Operator Precedence Grammars to construct an Yacc-like parser constructor named
PAPAGENO, which generates parallel parsers. The authors managed to describe a
grammar for Lua and JSON parsing, which they used in their tests. The authors
managed a speedup of up to $5.5\times$ when compared to a parser generated by
GNU Bison.

As for parallel compilation \textit{de facto}, there are two relevant works by
\cite{vandevoorde1988parallel} for C , and \cite{wortman1992} for Modula-2+.
The first assumes that every function declaration is in the file headers, but
implements per-function and per-statement parallel compilation, and the second
implements only per-function parallelism.  Speedups ranged from $1.5\times$ to
$6\times$ on a multicore MicroVAX II machine. None of these papers discuss
optimization.

There has been an attempt of parallelizing GCC by threading the GIMPLE
Intraprocedural pass manager \cite{bernardino2020improving}, which resulted in
a speedup of up to $3.35\times$ to this compilation stage, but up to $1.88\times$ in
total compilation of a file when extending this technique to the RTL passes.


\subsection{Link Time Optimization (LTO)} \label{lto_section}

The Classical Compilation scheme loads the content of a module (or file) through
lexical and syntactic analysis, then optimize the code, translate to assembler, and
encapsulate it in an object file. The issue with this scheme is that it can not
see the content of functions outside current module.

As an answer to this, LTO allows cross module optimizations by
postponing optimizations and final translation to a linker wrapper. There, the entire
program can be loaded by the compiler (but more often, just some sort of summary)
and optimizations can be decided globally, as now it has access to other modules. LTO is divided in three steps
\cite{whoprgoogle,glek2010optimizing}:
\begin{itemize}
\item LGEN (\textit{Local Generation}): each module is translated to an Intermediate
Representation (IR) and written to disk in phony object files. These objects do
not contain assembler code, and therefore can not be linked by an default linker
such as \textit{ld} (unless fat objects are enabled, in this case assembler is also
generated and dumped together with the IR). This phase runs serially
on input file (\textit{i.e.} in parallel with regard to the files in the project).

\item WPA (\textit{Whole Program Analysis}): load all translated module and analyze
the program globally by merging all Compilation Units into one. A Compilation Unit
is the entire content of a source file (a .c file in C) plus all headers it includes.
Then it generates a log of transformations for the program, and this global
Compilation Unit is partitioned for the next phase. This analysis runs sequentially
to the entire project.

\item LTRANS (\textit{Local Transformations}): apply the transformations generated by
WPA to each partition. Each partition will generate its own object file, which will
have to be linked together in the future. This phase runs in parallel.
\end{itemize}

This process is sketched in Fig. \ref{fig:whopr_build}, where the linker wrapper is
represented by \textit{collect2}, which firstly launch \textit{lto1} in WPA mode, and the second
time it finally launches \textit{ld}. This process can be seen by launching gcc with \texttt{-flto -v}.

\begin{figure}
\tikzstyle{block} = [rectangle, draw, fill=white,
    text width=6em, text centered, rounded corners, node distance=1cm and 0.5cm, minimum height=2em]
\tikzstyle{line} = [draw, -latex]
\makebox[\textwidth][c]{

\scalebox{0.8}{
\begin{tikzpicture}[node distance = 3cm, auto]
    % Place nodes
    \node [block]              (fonte1) {source1.c};
    \node [block, right= of fonte1]        (fonte2) {source2.cpp};
    \node [block, right= of fonte2]        (fonte3) {source3.f90};
    \node [block, above= of fonte2]         (make)   {Makefile};

    \node [block, below= of fonte1]        (gcc)      {gcc};
    \node [block, below= of fonte2]        (g++)      {g++};
    \node [block, below= of fonte3]        (gfortran) {gfortran};

    \node [block, below= of gcc]           (objeto1) {obj1.o};
    \node [block, below= of g++]           (objeto2) {obj2.o};
    \node [block, below= of gfortran]      (objeto3) {obj3.o};

    \node [block, below= of objeto2]       (gcc_lto) {collect2 (lto1)};

    \node [block, right= of fonte3]            (gcc_ltrans1) {gcc\_ltrans};
    \node [block, right= of gcc_ltrans1]   (gcc_ltrans2) {gcc\_ltrans};
    \node [block, right= of gcc_ltrans2]   (gcc_ltrans3) {gcc\_ltrans};

    \node [block, above= of gcc_ltrans2]       (gcc_wpa) {gcc\_wpa};
    \coordinate[below= of gcc_wpa]            (c2);


    \node [block, below= of gcc_ltrans1]   (obj1) {obj1.o};
    \node [block, below= of gcc_ltrans2]   (obj2) {obj2.o};
    \node [block, below= of gcc_ltrans3]   (obj3) {obj3.o};

    \node [block, below=of obj2]   (ld) {collect2 (LD)};

	\node [block, below=of ld]   (bin) {Binary};

    % Draw edges
    \draw[->]    ([xshift=-0.7em] make.south)   -- (fonte1.north);
    \draw[->]    (make.south)   -- (fonte2.north);
    \draw[->]    ([xshift=+0.7em] make.south)   -- (fonte3.north);

    \draw[->]    (fonte1.south)   -- (gcc.north);
    \draw[->]    (fonte2.south)   -- (g++.north);
    \draw[->]    (fonte3.south)   -- (gfortran.north);

    \draw[->]    (gcc.south)   -- (objeto1.north);
    \draw[->]    (g++.south)   -- (objeto2.north);
    \draw[->]    (gfortran.south)   -- (objeto3.north);

    \draw[->]    (objeto1.south)   -- ([xshift=-0.7em]gcc_lto.north);
    \draw[->]    (objeto2.south)   -- (gcc_lto.north);
    \draw[->]    (objeto3.south)   -- ([xshift=+0.7em]gcc_lto.north);

    %\draw[->]    (gcc_lto.south)   -- (gcc_wpa.north);
 	\draw[->]  (gcc_lto.east) .. controls +(6.5,0) and +(-6.5,0).. (gcc_wpa.west);

    \draw[->]    (gcc_wpa.south)   -- (gcc_ltrans1.north);
    \draw[->]    (gcc_wpa.south)   -- (gcc_ltrans2.north);
    \draw[->]    (gcc_wpa.south)   -- (gcc_ltrans3.north);

    \draw[->]    (gcc_ltrans1.south)   -- (obj1.north);
    \draw[->]    (gcc_ltrans2.south)   -- (obj2.north);
    \draw[->]    (gcc_ltrans3.south)   -- (obj3.north);

    \draw[->]    (obj1.south)   -- ([xshift=-0.7em]ld.north);
    \draw[->]    (obj2.south)   -- (ld.north);
    \draw[->]    (obj3.south)   -- ([xshift=+0.7em]ld.north);

	\draw[->]    (ld.south)   -- (bin.north);
	
	%draw brackets
\draw [decorate,decoration={brace,amplitude=10pt},xshift=-0.5cm,yshift=0pt]
([xshift=-1.3cm]objeto1.south) -- ([xshift=-1.3cm]fonte1.north) node [black,midway,xshift=-0.3cm]
{\footnotesize \begin{turn}{90}LGEN\end{turn}};

\draw [decorate,decoration={brace,amplitude=10pt},xshift=-0.5cm,yshift=0pt]
([xshift=1.3cm]gcc_ltrans3.north) -- ([xshift=1.3cm]obj3.south) node [black,midway,xshift=0.3cm]
{\footnotesize \begin{turn}{-90}LTRANS\end{turn}};


\end{tikzpicture}
}
}%
\caption{Compilation of a program using LTO scheme}
\label{fig:whopr_build}
\end{figure}

\section{A Middle Ground} \label{sec:work}

As presented in Section \ref{lto_section}, LTO was created to allow expensive
optimizations in industrial sized programs, and has a serial part (WPA), which
might impose a bottleneck on manycore machines. Classical Compilation scheme,
however, can not partition its Compilation Unit for parallel compilation, which can also
bottleneck the compilation if it is too large. Therefore, can we transplant the
LTO partitioner to the Classical Compilation scheme, and make it work
\textit{without} having the context of the \textit{entire} program?
The answer is \textit{yes}, and we show that by showing the details
of our implementation in GCC.

Our approach differs from LTO mainly in how we handle the Interprocedural
Analysis. LTO handles them with the context of the whole program, while
our approach will only have the context of the original Compilation Unit.
This allows optimizations as good as they are in the Classical
Compilation scheme, while benefiting of the extra parallelism opportunity
available in the LTO's LTRANS stage.

In this section, we will first discuss the internals of some parts of
GCC, which we had to modify for our implementation to work. First,
we present an important piece of the compiler from the User Experience perspective:
the \texttt{gcc} \textit{driver}.
Then we present a short algorithm for making the LTO partitioner
work for our proposal. We then present a necessary change we had to do
in our work about how partitions are applied in GCC, and finally, we
discuss about how to solve Name Clashing without having the context of the
entire program.

\subsection{The GCC driver}

An large program can be written in several languages, with each of them having
its own compiler. From a Compiler Theory perspective, a compiler is a software
that translates a program from a language $A$ to an language $B$
\cite{dragonbook}.  In the case of GCC, it translates several languages, such
as C, to Assembler of some architecture, such as x86. This means that
encapsulating code in object files, or linking the code in an executable, are
not tasks of the compiler. However, the user can launch \texttt{gcc -o binary
file.c} and get a working binary. That is because the driver will launch the
necessary programs for the user, and in fact this line launches three programs,
as illustrated in Fig. \ref{fig:gnu_toolchain}.

\begin{figure}
\tikzstyle{block} = [rectangle, draw, fill=white,
    text width=6em, text centered, rounded corners, node distance=4.7cm, auto, minimum height=2em]
\tikzstyle{line} = [draw, -latex]
\tikzstyle{cloud} = [draw, ellipse,fill=white, node distance=2cm,
    minimum height=2em]
\makebox[\textwidth][c]{
\scalebox{0.8}{
\begin{tikzpicture}[node distance = 3cm, auto]
    % Place nodes
    \node [block]                      (cc1) {Compiler \\ (cc1)};
    \node [block, right of = cc1]      (as) {Assembler\\(as)};
    \node [block, right of = as]       (ld) {Linker\\collect2 or ld};
    \coordinate [left of=cc1]          (fonte);
    \coordinate [right of=ld]    (bin);

    % Draw edges
    \draw[->]    (cc1.east)    -- (as.west)       node[midway, above] {Assembler File};
    \draw[->]    (cc1.east)    -- (as.west)       node[midway, below] {(.s)};
    \draw[->]    (as.east)     -- (ld.west)       node[midway, above] {Object File};
    \draw[->]    (as.east)     -- (ld.west)       node[midway, below] {(.o)};
    \draw[->]    (fonte.west)  -- (cc1.west)      node[pos=0, above] {Source File};
    \draw[->]    (fonte.west)  -- (cc1.west)      node[pos=0, below] {(.c)};
    \draw[->]    (ld.east)     -- (bin.west)      node[pos=1, above] {Executable};
\end{tikzpicture}
}
}%
\caption{GCC Compiling a .c file in Classical Compilation mode}
\label{fig:gnu_toolchain}
\end{figure}

Therefore, our changes should not break the building scripts (\textit{e.g.} Makefile)
used by most
projects, which is mostly launching \texttt{gcc file.c -c}, which creates
an object file \texttt{file.o}. However, LTO partitioner will create an object file for each partition. This
would result in multiple object files which will be needed by the final
link process, which imposes a compatibility problem. The solution for it is:
\begin{enumerate}
	\item Patch the \textit{partitioner} to communicate the location of
	each files created to the \textit{driver}. If the \textit{partitioner}
	is the compiler (which is the case of GCC), then it should communicate
	the location of each generated \textit{assembler file}. This can be
	archived by passing a hidden flag \texttt{-fadditional-asm=<file>}
	by the driver to the partitioner, which the last will write to. This file can also
	be replaced with a Named Pipe for better performance, if needed.

	Then, the partitioner checks if this flag has been passed to the compiler. If yes, then
	a \textit{compatible version} of the driver is installed. If the
	partitioner decides to partition the Compilation Unit, it should
	\textit{retarget} the destination assembler file, and write the retarget
	name to the communication file.

	\item Patch the driver to pass this hidden flag to the
	\textit{partitioner}.  Then check if this file exist. If not, it means that
	either the compiler is incompatible (assuming it did not halt with an
	error) or it has chosen to not partition the Compilation Unit. In the first
	case, the driver should call \textit{as} to every assembler file generated, and call
	the linker to generate the expected final object file. In GNU \textit{ld},
	it is necessary to enable the \textit{partial linking} flag for linking
	object files into another object file. In the second case, simply fallback
	to the previous compilation logic.
\end{enumerate}

Fig. \ref{fig:gnu_toolchain_patched} illustrates the code flow after these
changes.  The execution starts in the highlighted node \textit{Driver}, which
calls the compiler with the necessary flag to stablish a communication between
the parts. The compiler then will partition the Compilation Unit and forks
itself into several child processes, one for each partition. Then the compiler
will communicate its output .s file to the driver. The driver then will launch the
\textit{as} to assemble it, and then launch \textit{ld} to merge them all into
a single object file.

After these changes, a good way to check if the changes are working is to
bootstrap the compiler with a single partition, but writing the output
path into the communication channel. Bootstrapping can be an resource
intensive task, therefore this is an excellent time to write automated
tests covering every case necessary for the bootstrap. This may also expose
some extreme cases, for instance, the C compiler being called to preprocess an
file of another language.

\begin{figure}
\tikzstyle{block} = [rectangle, draw, fill=white,
    text width=6em, text centered, rounded corners, minimum height=2em]
\tikzstyle{line} = [draw, -latex]

\makebox[\textwidth][c]{
\scalebox{0.8}{
\begin{tikzpicture}[node distance = 2.4cm, auto]
    % Place nodes
    \node [block]                      (cc1_1) {cc1};
    \coordinate [right= of cc1]          (c);
    \node [block, right= of c,fill={rgb:black,1;white,2}]        (driver1) {Driver};
    \node [block, above= of c]                      (cc1_2) {cc1};
    \node [block, below= of c]                      (cc1_3) {cc1};
    \coordinate [above= of driver1]          (c2);
    \coordinate [below= of driver1]          (c3);
    \node [block, right= of c2]      (as2) {as};
    \node [block, right= of c3]      (as3) {as};
    \node [block, right= of driver1]       (ld) {ld};
    \coordinate [left= of cc1]          (fonte);
    \coordinate [right= of ld]    (bin);

    % Draw edges
    \draw[->]    (driver1.west)    -- (cc1_1.east) node[midway, above] {\texttt{-fadditional-asm=<file>}};
    \draw[->]    ([xshift=+0.5cm]cc1_2.south) -- ([xshift=-0.5cm]driver1.north) node[midway, above, sloped] {Generates .s file};
    \draw[->]    ([xshift=+0.5cm]cc1_3.north) -- ([xshift=-0.5cm]driver1.south) node[midway, above, sloped] {Generates .s file};

    \draw[->]    (cc1_1.north)    -- ([xshift=-0.5cm]cc1_2.south) node[midway, above, sloped] {Forks};
    \draw[->]    (cc1_1.south)    -- ([xshift=-0.5cm]cc1_3.north) node[midway, above, sloped] {Forks};

    \draw[->]    ([xshift=+0.5cm]driver1.north)  -- (as2.south) node[midway, above, sloped] {Assemble received .s};
    \draw[->]    ([xshift=+0.5cm]driver1.south)  -- (as3.north) node[midway, above, sloped] {Assemble receibed .s};
    \draw[->]    (driver1.east)  -- (ld.west) node[midway, above, sloped] {Links};

%    \draw[->]    (cc1.east)    -- (as.west)       node[midway, above] {Assembler File};
%    \draw[->]    (cc1.east)    -- (as.west)       node[midway, above] {Assembler File};
%
%    \draw[->]    (cc1.east)    -- (as.west)       node[midway, below] {(.s)};
%    \draw[->]    (as.east)     -- (ld.west)       node[midway, above] {Object File};
%    \draw[->]    (as.east)     -- (ld.west)       node[midway, below] {(.o)};
%    \draw[->]    (fonte.west)  -- (cc1.west)      node[pos=0, above] {Source File};
%    \draw[->]    (fonte.west)  -- (cc1.west)      node[pos=0, below] {(.c)};
    \draw[->]    (ld.east)     -- (bin.west)      node[pos=1, above] {\texttt{file.o}};
\end{tikzpicture}
}
}%
\caption{Interaction between the \textit{driver}, the \textit{compiler}, and other tools
after our changes}
\label{fig:gnu_toolchain_patched}
\end{figure}

\subsection{Adapting the LTO Partitioner}

In GCC, a Compilation Unit is represented as a callgraph. Every function,
global variable or their clones (which may represent a function to be inlined)
are represented as nodes in a callgraph. If there is a function call from $f$
to $g$, then there is an edge from $f$ to $g$. Similarly, if there is an
reference to an global variable $v$ in $f$, then there is also an edge from $f$
to $v$. This means that Compilation Unit partitioning is simply represented as
a Graph Partitioning problem. 

When LTO is enabled and GCC is in WPA stage, the code content of these nodes
are not present, just a \textit{summary} of it (\textit{e.g.}the size
in lines of code it had). This is done to conserve memory. However,
when LTO is disabled, the contents of the function is present, and this
resulted in some assertions failure, which were fixed after some debugging.

Then comes the partitioner algorithm \textit{de facto}. In LTO, GCC tries to create
partitions of similar size, and always try to keep nodes together. The
heuristic used there is quite complicated to run in linear time, and turned
out to be complex to find the issues that were causing the partitioning to fail.
Therefore, we decided to design a new partitioning algorithm for this project.

This partitioning algorithm works as follows: For each node, we check if they
have some desired property, such as being a member of a COMDAT group, and
merged into the same partition. We then propagate outside of this COMDAT group
checking for every node that may trigger the COMDAT to be copied into other
partitions, and also add them to the same partition. In practice,
this means to include every node hit by a Depth-First
Search (DFS) from the group to an non-cloned node outside of the group.
Fig. \ref{fig:comdat_frontier} represents a sketch of this process.

\begin{figure}
\centering
	 \includegraphics[scale=0.8]{figuras/comdat_frontier.pdf}
	  \caption{Example of callgraph, in beige being represented the COMDAT group,
	  in green the COMDAT frontier, and in red the cloned nodes.}
	  \label{fig:comdat_frontier}
\end{figure}

At first we we also did this process for private functions to avoid having to
promote them then to public, once external access would be necessary if they go
into distinct partitions. However, results showed that this have a strong
negative hit in any parallelism opportunity. For grouping the nodes together,
we used an Union Find with Path Compression, which yields an attractive
computational complexity of $O(E + N \lg^*N)$ to our partitioner, where $N$ is the
number of nodes and $E$ is the number of edges in the callgraph \cite{feufiloff}.

Once the partitions are computed, we need to compute its \textit{boundary}
If function $f$ calls $g$, but they were assigned into distinct partitions,
then we must include a version of $g$ in $f$'s partitions without its body,
then check if $g$ is a private function. If yes, them $g$ must be promoted
to a public function. There is also extra complexity if a version of $g$
is marked to be inlined in $f$, which means that its body has to be
streamed somehow. Fortunately, most of this logic is already present
in LTO and we could simply reuse them. However, some issues were found
when handling inline functions and global variables marked as being part
of the boundary. The first being some functions marked to be inlined into 
functions inside the partition were incorrectly marked to be removed.
The second being variables marked as in the boundary (and therefore
not in the partition) not being correctly promoted to global. These issues
were hard to find the reason of, but easy to fix.

Furthermore, there were some issues with
regard to how GCC handle partitions, which we discuss in the next subsection.

\subsection{Applying a Partition Mask}

Once partitions are computed, the only presented way to apply it (\textit{i.e.}
remove every unnecessary node from the Compilation Unit) was to reload the
compiler, and let it load the phony object files, which are not available in our
project because we are not running in LTO mode. We developed another method for this.
We used the Unix \textit{fork} function to spawn a child process, and then
we implemented our own method to apply the partition without having to load
there phony object files. Fortunately, this consisted
of four cases:
\begin{itemize}
	\item Node is in partition. Nothing has to be done.
	\item Node is in boundary: We mark this mode as having its body removed,
	but we never actually do remove it. This is because the node may share the
	body contents with another node in the partition. We then remove
	every edge from its functions to it callees, all references to variables,
	the content of the dominator tree, and also its Control-Flow Graph. This
	is now a function which this partition only know that it exists.
	
	\item Node is in boundary, but keep its body: We just set a set of
	attributes to false, but we do not do anything too drastic about it.

	\item Node not in boundary: We remove the node, and all its content.
\end{itemize}

After this, it retargets the output assembler file to another file,
and write its partition number together with the path to the communication
file, which the driver will read in the future. The partition number is
important to guarantee that the build is reproducible, as we will discuss
later.

It is important that some early step in the compiler does not emit assembler
too early, as were the issue with the Gimplifier in GCC, or else the output
file will be incomplete. We had to fix the gimplifier to avoid that.

Once the partition is applied to the current process, then it can
continue with the compilation. It should be running in parallel
now by the number of partitions.

\section{Name Clash Resolution}

In LTO, if there are two promoted symbols to global with the same name, it is
quite straightforward to fix this issue. Since we have the context of the
entire program, we can simply increment a counter for each clashed symbol and
rename them. However, we have not the context of the entire program, and we
have to find another way of fixing it.

One will be tempted to simply select an random integer and append to the name.
This is not a good idea, once it will break bootstrap because each
compilation will result in a different object file. Using the address of the
function in memory does not work either, as the first bootstrap step has the
compiler compiled with \texttt{-O0} and the second step with \texttt{-O2}, and
memory address will certainly be different.

The solution is to
use a crc32 hash of the original file and append them to the function's name,
since we can not have two functions with the same (mangled) name in the program.
There is still a probability of name clash with this approach, however we did
not find any on our tests.

\section{Integration with GNU Jobserver}

GNU Make is able to launch jobs in parallel by using \texttt{make -j}. This
is so simple and yet so powerful that even recent building scripts (such as CMake)
relies on Make for launching jobs.

In order to avoid unnecessary partitioning and job creation, we have also
implemented a integration mechanism with GNU Jobserver \cite{posixjobserver}.
The implementation
is simple: we query the server for an extra token. If we receive the token,
then it means that some processor is available, and we can partition the
Compilation Unit and launch jobs inside the compiler. Else, the processor
workload is full, and it may be better to avoid partitioning altogether.

However, for a program to be able to communicate with the jobserver,
it should be launched with a prepended \texttt{+} character,
for example $\texttt{+gcc -c -O2 file.c}$, and therefore it is not
so straightforward to use this mode on existing projects.

\section{Relationship with Reproducible Builds}

Open Source can be verified by everyone, but what ensures that the binary
being distributed by a certain developer is, in fact, from a codebase
that was not changed with malicious intent?

The Reproducible Builds aims to solve that issue by providing a way to
reproduce the released binary from its source. Some software needs to
be patched in order to work with Reproducible Builds, for instance,
to not contain some kind of build timestamp, and so on. A build
is called \textit{reproducible} if given the same sourcecode and build
instructions, anyone can recreate a bit-perfect version of the distributed
binary \cite{reproducible_builds}.

To keep compatibility with this project, we must ensure that our compiler
always output the same code with a given input. The input is not only
the source file itself, but also the flags passed to it.

We claim that our builds are reproducible because of the following reasons:

\begin{enumerate}
	\item No random information is necessary to solve name clashing.
	\item Given a number of jobs, our partitioner will always generate
	the same number of partitions for a certain program, always with the same content.
	\item Partial Linking is always done in the same order. To ensure that,
	we communicate to the driver a pair (\textit{partition number, path to file}),
	and we sort this list using the partition number as key.
	\item No other race conditions are introduced, as we rely on the quality of
	the LTO implementation of the compiler.
\end{enumerate}

However, there is one point of concern, which is the Jobserver
Integration.  If the processor jobs is already in 100\% usage, we avoid
partitioning at all and proceed with sequential compilation. This certainly
changes from computer to computer, and therefore the build is not reproducible
if this option is enabled. This is not an issue if the number of jobs is
determined beforehand.

\section{Methods and Current Issues}\label{sec:methods}

We ensure the correctness of our changes by:
\begin{enumerate}
	\item Bootstrap GCC with the number of parallel jobs as 2, 8 and 64, with
	minimum partitioning quota of $10^3$ and $10^5$ instructions. We found issues
	with regard to how the \textit{ipa-split} pass generated clones, and therefore
	we have disabled it for now. This pass breaks big functions
	into a header and a body, and marks the header to be inlined to the caller.
	A bug in our implementation marked such clones as being an external function in
	rare cases, which the linker can not find.

	\item Run the GCC testsuite, which we noticed that all tests related to the debug
	symbols were failing. However, if \texttt{-g0} is passed, no debug symbol is created,
	and the bug related to this issue is never triggered. The reason behind this is
	because our partition applier do not remove the symbols associated with removed
	nodes, resulting in unknown symbol being dumped into assembler.

	\item Generated random programs with \textit{csmith} \cite{yang2011finding}. This found more complicated
	bugs (for instance, long strings being output before the assembler file retarget),
	which we fixed.
\end{enumerate}

Then for the time measurements, we sampled $n = 15$ points for each file from
the GCC project, which we present the average in the graphics. The errorbars
represents 95\% confidence interval for the sample average. Furthermore, each
file were preprocessed by the C preprocessor, so it could be compiled independently
of other files.

For the projects, each point represents a mean were compiled with $n = 5$ points,
because this is a more computer intensive task. The errorbars also represent a 95\%
confidence interval of the average. We also were sure that the serial data was also
compiled with \texttt{-g0} for fairness.

Tests were mainly executed in two computers, which are represented
in the Table \ref{table:machines}. The graphic caption specify where the test
was run.

The version of GCC used in the tests is available in the \texttt{autopar\_europar\_2021}
branch of \texttt{git://gcc.gnu.org/git/gcc.git}, with hash \texttt{e2da2f7205}.


\begin{table}[]
\begin{tabular}{c|c|c|c|c|}
\cline{2-5}
                                      & Number of Cores & Number of Threads & RAM                                                    & Disk Type \\ \hline
\multicolumn{1}{|c|}{Core-i7 8650U}   & 4               & 8                 & \begin{tabular}[c]{@{}c@{}}8Gb \\ DDR4\end{tabular} & SSD       \\ \hline
\multicolumn{1}{|c|}{4x Opteron 6376} & 32              & 64                & \begin{tabular}[c]{@{}c@{}}128Gb\\ DDR3\end{tabular}   & HDD       \\ \hline
\end{tabular}
\caption{Machine specification of tests}
\label{table:machines}
\end{table}

\section{Results}\label{sec:results}

We first highlight our best results. We managed a $2.4\times$ and
$2.25\times$ speedup on the Core-i7 machine when compiling the files
\textit{gimple-match.c} and \textit{insn-emit.c}, which are autogenerated
files from the GCC project, and takes quite long to compile
($76s$ and $23s$ sequential). For non-generated files, we have
\textit{tree-vect-stmt.c}, with a $2.14\times$ speedup and taking
$11s$ sequentially. All these
speedups were archived by using 8 parallel jobs. Fig. \ref{fig:gcc_all_files} shows the results globally. Here we can see that for files with $\textit{Number of Instructions} >
1 \times 10^5$, we have mostly significant improvements.

Now we move to the $4\times$ Opteron machine. We managed a
$3.32\times$, $3.53\times$ on \textit{gimple-match.c} and
\textit{insn-emit.c} by using 64 jobs. This is close to the
maximum theoretical speedup of $4\times$ computed by
\cite{bernardino2020improving} when parallelizing the
Intraprocedural Optimizations.

We will now discuss how these changes impacts the overall compilation
time of some projects. For this, we run experiments compiling
the Linux Kernel 5.19.6, Git 2.30.0, the GCC version mentioned in
Section Bla with and without bootstrap enabled, and JSON C++, with
commit hash \texttt{01f6b2e7418537}. We have also compiled GCC and
Git with the jobserver integration enabled, and the reason because
we did not enabled it in other projects is because it is necessary
to modify a absolutely large number of Makefiles (for instance, Linux
has 2597 Makefiles). 

In Fig. \ref{fig:gcc_projects} we show our results. We can observe
a near $35\%$ improvement when compiling GCC with bootstrap disabled,
and $25\%$ when bootstrap is enabled. We also managed a $15\%$
improvement when compiling Git, and no meaningful improvement
or slowdown in other projects. These tests were executed with 64 Makefile
jobs and 8 threads inside the compiler.

\begin{figure}
\centering
	 \includegraphics[scale=0.65]{figuras/times-insns.pdf}
	  \caption{Compilation of each file with 1, 2, 4, and 8 threads on
	  Core-i7}
	  \label{fig:gcc_all_files}
\end{figure}

\begin{figure}
\centering
	 \includegraphics[scale=0.65]{figuras/experiment_projects.pdf}
	  \caption{Compilation of some projects with 64 Makefile jobs and 8 threads in compiler}
	  \label{fig:gcc_projects}
\end{figure}

\section{Conclusions and Future Works}\label{sec:future_works}

We have shown a tangible way of compiling files in parallel capable
of being implemented in industrial compilers, which resulted in
speedups when compiling single files in parallel, as well as some
speedup when compiling entire projects in manycore machines. However,
There are several points in which our work can be improved.

First, is by fixing the problems reported in Section \ref{sec:methods}. These bugs
certainly prevents the current branch to be used in industrial environments (which is a
good reason why this was not merged in upstream yet), but they are fine as a
proof of concept to support our claims.

Second is by implementing a better partitioner to the project. One main issue
with our partitioner is that we kept its load balancing algorithm minimal to
ensure that it works. Using the LTO default partitioner as a base is a good start.

Third, by modifying the driver to also support external compiler through GCC
SPEC language. Current implementation only checks if launching program
is a known compier/assembler/linker, and will get confused in languages that
needs additional steps (such as CUDA).

And forth, try to develop a predictive model to decide if the input file is
a good candidate for parallel compilation. Fig. \ref{fig:gcc_all_files} shows
a clear linear correlation between the expected number of instructions and time
(and maybe it is the best parameter), but it may be possible to (statically)
collect more information about the file for a better decision.

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
%\begin{thebibliography}{8}
%\bibitem{ref_article1}
%Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)
%
%\bibitem{ref_lncs1}
%Author, F., Author, S.: Title of a proceedings paper. In: Editor,
%F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
%Springer, Heidelberg (2016). \doi{10.10007/1234567890}
%
%\bibitem{ref_book1}
%Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
%Location (1999)
%
%\bibitem{ref_proc1}
%Author, A.-B.: Contribution title. In: 9th International Proceedings
%on Proceedings, pp. 1--2. Publisher, Location (2010)
%
%\bibitem{ref_url1}
%LNCS Homepage, \url{http://www.springer.com/lncs}. Last accessed 4
%Oct 2017
%\end{thebibliography}

\bibliographystyle{splncs04}
\bibliography{bibliography}

\end{document}
